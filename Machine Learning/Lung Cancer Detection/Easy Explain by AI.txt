এই কোডটি একটি লাং ক্যান্সার শনাক্তকরণ মডেল তৈরি করতে ব্যবহৃত হয়েছে। এটি বিভিন্ন মেশিন লার্নিং মডেল ব্যবহার করে লাং ক্যান্সার শনাক্ত করার জন্য প্রশিক্ষিত হয়। এখানে প্রতিটি ধাপের বিস্তারিত ব্যাখ্যা দেওয়া হলো:

1. লাইব্রেরি আমদানি:
কোডটি প্রথমে প্রয়োজনীয় প্যাকেজগুলি ইমপোর্ট করে:

numpy, pandas: ডেটা ম্যানিপুলেশন ও বিশ্লেষণের জন্য।
matplotlib, seaborn: ডেটা ভিজ্যুয়ালাইজেশন (গ্রাফ, চার্ট ইত্যাদি)।
sklearn: বিভিন্ন মেশিন লার্নিং মডেল এবং পরিমাপের জন্য।
2. ডেটা লোড ও প্রক্রিয়াজাতকরণ:
files.upload(): গুগল কোল্যাবে ফাইল আপলোড করার জন্য ব্যবহার করা হয়েছে।
ডেটা ফাইলটি pd.read_csv() ব্যবহার করে প্যান্ডাস ডেটাফ্রেমে লোড করা হয়েছে।
df.head(7): প্রথম ৭টি সারি দেখানো হয়েছে।
df.shape: ডেটাসেটের আকার (সারি ও কলাম সংখ্যা) দেখানো হয়েছে।
df.describe(): ডেটাসেটের বর্ণনামূলক পরিসংখ্যান (যেমন গড়, স্ট্যান্ডার্ড ডেভিয়েশন) দেখানো হয়েছে।
3. ডেটা প্রক্রিয়াকরণ:
df.set_index('index'): 'index' কলামটি ডেটাফ্রেমের ইনডেক্স হিসেবে সেট করা হয়েছে।
df['Level'].map(level_mapping): 'Level' কলামের মানগুলি মেপিংয়ের মাধ্যমে সংখ্যা (1, 2, 3) হিসেবে রূপান্তরিত করা হয়েছে, যেখানে 'Low', 'Medium', 'High' মানগুলোর জন্য যথাক্রমে 1, 2, 3।
df.isnull().sum(): ডেটাফ্রেমে যে সমস্ত নাল মান রয়েছে, তা গণনা করা হয়েছে।
4. ভিজ্যুয়ালাইজেশন:
হিস্টোগ্রাম: df.hist() ব্যবহার করে প্রতিটি বৈশিষ্ট্যের জন্য হিষ্টোগ্রাম প্লট করা হয়েছে।
কোরেলেশন ম্যাট্রিক্স: sns.heatmap(cor) ব্যবহার করে বৈশিষ্ট্যগুলির মধ্যে সম্পর্কের হিটম্যাপ প্লট করা হয়েছে।
5. ডেটা সেগমেন্টেশন:
লক্ষ্য ও ইনপুট ভ্যারিয়েবল নির্ধারণ:
Y = df['Level'].values: 'Level' কলামটি লক্ষ্য ভ্যারিয়েবল হিসেবে নির্ধারণ করা হয়েছে।
X = df.drop('Level', axis=1).values: বাকি সকল কলাম ইনপুট বৈশিষ্ট্য (features) হিসেবে নেওয়া হয়েছে।
ডেটাকে ট্রেনিং ও টেস্টিং সেটে ভাগ করা হয়েছে train_test_split() ব্যবহার করে (টেস্ট সাইজ ২০% এবং র্যান্ডম স্টেট ২১)।
6. মডেল ট্রেনিং:
মডেল নির্বাচন: কোডে ৪টি মডেল ব্যবহৃত হয়েছে:
DecisionTreeClassifier (CART)
SVC (Support Vector Classifier)
GaussianNB (Naive Bayes)
KNeighborsClassifier (KNN)
ক্লাসিফিকেশন মেট্রিক্স: প্রতিটি মডেলের জন্য ১০ ফোল্ড ক্রস ভ্যালিডেশন (cross-validation) করা হয়েছে এবং তাদের গড় সঠিকতা (accuracy) প্রদর্শন করা হয়েছে।
7. স্ট্যান্ডারাইজড ডেটা:
StandardScaler ব্যবহার করে মডেলগুলির প্রশিক্ষণের পূর্বে ডেটাকে স্কেল করা হয়েছে, যাতে মডেলগুলি সঠিকভাবে কাজ করতে পারে।
ক্লাসিফিকেশন মেট্রিক্স পুনরায় হিসাব করা হয়েছে স্ট্যান্ডারাইজড ডেটার জন্য।
8. প্রেডিকশন এবং ফলাফল:
প্রতিটি মডেলের জন্য model.fit() ব্যবহার করে মডেল প্রশিক্ষণ করা হয়েছে এবং model.predict() ব্যবহার করে টেস্ট ডেটার জন্য প্রেডিকশন করা হয়েছে।
মডেলগুলির Accuracy, Classification Report, এবং Confusion Matrix মুদ্রিত হয়েছে, যা মডেলটির কার্যকারিতা পরিমাপ করতে সাহায্য করে।
9. কনফিউশন ম্যাট্রিক্স:
plot_confusion_matrix() ফাংশন ব্যবহার করে কনফিউশন ম্যাট্রিক্স প্লট করা হয়েছে, যা মডেলের পারফরম্যান্সকে ভিজ্যুয়ালি উপস্থাপন করে।
10. মডেল সংরক্ষণ:
pickle.dump(clf, open('model.pkl','wb')): মডেলটি model.pkl ফাইল হিসেবে সংরক্ষণ করা হয়েছে, যাতে ভবিষ্যতে এটি লোড করে ব্যবহার করা যায়।
pickle.load(open('model.pkl','rb')): পূর্বে সংরক্ষিত মডেলটি লোড করে নতুন ডেটার প্রেডিকশন করা হয়েছে।
11. উদাহরণ দিয়ে প্রেডিকশন:
example_measures: একটি উদাহরণ দেওয়া হয়েছে, যেখানে নতুন কিছু মাপ নিয়ে মডেলটি প্রেডিকশন করছে। এখানে মডেলটি পরীক্ষা করছে যে এই উদাহরণের ফলাফল ক্যান্সার রয়েছে কি না।
সারাংশ:
এই কোডটি মূলত লাং ক্যান্সার ডিটেকশন করার জন্য বিভিন্ন মেশিন লার্নিং মডেল (যেমন SVM, Decision Tree, Naive Bayes, KNN) ব্যবহার করে প্রশিক্ষণ ও টেস্টিং করে। এর মাধ্যমে ক্যান্সার প্রেডিকশন তৈরি করা এবং এর কার্যকারিতা যাচাই করা হচ্ছে। শেষে মডেলটি সংরক্ষণ করা হচ্ছে, যাতে ভবিষ্যতে ব্যবহৃত হতে পারে।


# -*- coding: utf-8 -*-
"""lung cancer detect custom.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KUrKktH2TGEa78mW9Ca4nLvZi8rjVb8f
"""

# প্রয়োজনীয় লাইব্রেরি গুলি ইমপোর্ট করা হচ্ছে
import numpy as np  # ন্যুমপাই লাইব্রেরি ব্যবহার করা হবে ম্যাথমেটিক্যাল অপারেশনগুলোর জন্য
import pandas as pd  # প্যান্ডাস লাইব্রেরি ব্যবহার করা হবে ডেটা ম্যানিপুলেশন ও অ্যানালাইসিসের জন্য
import matplotlib.pyplot as plt  # গ্রাফ আঁকতে ম্যাটপ্লটলিব লাইব্রেরি
from sklearn.metrics import classification_report  # ক্লাসিফিকেশন রিপোর্ট তৈরির জন্য
from sklearn.metrics import confusion_matrix  # কনফিউশন ম্যাট্রিক্স তৈরি করার জন্য
from sklearn.metrics import accuracy_score  # মডেলের সঠিকতা (accuracy) নির্ধারণের জন্য
from sklearn.model_selection import train_test_split  # ডেটা ভাগ করার জন্য
from sklearn.model_selection import cross_val_score  # ক্রস ভ্যালিডেশন স্কোর বের করার জন্য
from sklearn.model_selection import KFold  # কফোল্ড ক্রস ভ্যালিডেশনের জন্য
from sklearn.tree import DecisionTreeClassifier  # ডিসিশন ট্রি ক্লাসিফায়ার মডেল
from sklearn.neighbors import KNeighborsClassifier  # কেএনএন (KNN) ক্লাসিফায়ার মডেল
from sklearn.naive_bayes import GaussianNB  # গাউসিয়ান নায়ভ বায়েস ক্লাসিফায়ার মডেল
from sklearn.pipeline import Pipeline  # পাইপলাইন তৈরির জন্য
from sklearn.preprocessing import StandardScaler  # ডেটা স্কেল করার জন্য
from sklearn.model_selection import GridSearchCV  # গ্রিড সার্চ সিভি, মডেল প্যারামিটার টিউনিংয়ের জন্য
from sklearn.svm import SVC  # সাপোর্ট ভেক্টর ক্লাসিফায়ার (SVC) মডেল
import seaborn as sns  # সিবর্ন গ্রাফ তৈরির জন্য

# %matplotlib inline # এটি গ্রাফ প্রদর্শন করার জন্য ব্যবহৃত, তবে এখানে কমেন্ট করা হয়েছে।

# গুগল কোল্যাব থেকে ফাইল আপলোড করার জন্য কোড
from google.colab import files  # গুগল কোল্যাবের ফাইল আপলোড লাইব্রেরি
uploaded = files.upload()  # ফাইল আপলোড করা হচ্ছে
df = pd.read_csv('data.csv')  # CSV ফাইলটি প্যান্ডাস ডেটাফ্রেমে লোড করা হচ্ছে
df.head(7)  # প্রথম ৭টি সারি দেখানো হচ্ছে, যাতে ডেটা দেখতে পারেন

# ডেটাসেটের আকার দেখানো হচ্ছে (সারি এবং কলাম সংখ্যা)
df.shape  # আউটপুটে ডেটাসেটের সারি ও কলাম সংখ্যা দেখাবে

# ডেটাসেটের বর্ণনামূলক পরিসংখ্যান (mean, std, min, max ইত্যাদি) দেখা হচ্ছে
df.describe()  # আউটপুটে বিভিন্ন পরিসংখ্যান পরামর্শ দেখাবে

# 'index' কলামটিকে ইনডেক্স হিসেবে সেট করা হচ্ছে
df = df.set_index('index')  # 'index' কলামটি ইনডেক্স হিসেবে ব্যবহার করা হচ্ছে

# 'Patient Id' কলামটি যদি না প্রয়োজন হয়, তবে ড্রপ করা হয়েছে (কামেন্ট আউট করা)
# df = df.drop('Patient Id', axis=1)  # এই কলামটি ড্রপ করা হচ্ছে

# ডেটাসেটের সকল কলাম নাম দেখা হচ্ছে
df.columns  # সকল কলামের নাম দেখতে হবে

# 'Level' কলামের মানকে 'Low', 'Medium', 'High' থেকে সংখ্যা 1, 2, 3 তে রূপান্তর করা হচ্ছে
level_mapping = {'Low': 1, 'Medium': 2, 'High': 3}  # মানগুলোর জন্য একটি ম্যাপিং তৈরি করা হচ্ছে

# 'Level' কলামের মানগুলিকে উপরের ম্যাপিং অনুসারে রূপান্তর করা হচ্ছে
df['Level'] = df['Level'].map(level_mapping)  # 'Level' কলামের মানগুলো ম্যাপিংয়ের মাধ্যমে পরিবর্তন করা হচ্ছে

# 'Level' কলামটি পূর্ণসংখ্যা (int64) টাইপে রূপান্তর করা হচ্ছে
df['Level'] = df['Level'].astype('int64')  # টাইপ পরিবর্তন করা হচ্ছে

# 'Level' কলামের রূপান্তরের ফলাফল প্রদর্শন করা হচ্ছে
print(df['Level'])  # রূপান্তরিত 'Level' কলামটি প্রদর্শিত হবে

# 'Level' কলামে কী কী ভ্যালু রয়েছে তা দেখতে হবে (Benign বা Malignant)
print(df.groupby('Level').size())  # 'Level' এর ভিত্তিতে গ্রুপিং করে মোট সংখ্যা দেখানো হচ্ছে

# ডেটাসেটের কাঠামো (ধরণ, নাল মান ইত্যাদি) সম্পর্কে বিস্তারিত তথ্য দেখা হচ্ছে
df.info()  # ডেটাসেটের কাঠামো এবং ইনফরমেশন দেখা হবে

# প্রতিটি কলামে কতগুলি নাল মান রয়েছে তা দেখা হচ্ছে
missing_values_count = df.isnull().sum()  # নাল মান গণনা করা হচ্ছে
missing_values_count[0:10]  # প্রথম ১০টি কলামের নাল মান দেখানো হচ্ছে

# প্রতিটি ভেরিয়েবলের জন্য হিষ্টোগ্রাম তৈরি করা হচ্ছে
sns.set_style('darkgrid')  # সিবর্নের ডার্কগ্রিড স্টাইল সেট করা হচ্ছে
df.hist(figsize=(30,30))  # হিষ্টোগ্রাম তৈরি করা হচ্ছে
plt.show()  # গ্রাফটি প্রদর্শিত হবে

# কোরেলেশন ম্যাট্রিক্স (একটি ভেরিয়েবল কতটা অন্য ভেরিয়েবলের সাথে সম্পর্কিত) প্রদর্শন করা হচ্ছে
plt.figure(figsize=(30,20))  # গ্রাফের আকার সেট করা হচ্ছে
cor = df.corr()  # কোরেলেশন ম্যাট্রিক্স বের করা হচ্ছে
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)  # হিটম্যাপ প্লট করা হচ্ছে
plt.show()  # হিটম্যাপটি দেখানো হচ্ছে

# 'Level' কলামের সাথে অন্যান্য কলামের কোরেলেশন দেখানো হচ্ছে
cor_target = abs(cor["Level"])  # 'Level' এর সাথে অন্যান্য ভেরিয়েবলের কোরেলেশন নেওয়া হচ্ছে
relevant_features = cor_target[cor_target>0.7]  # যেসব ভেরিয়েবল কোরেলেশন > 0.7 তাদের নির্বাচন করা হচ্ছে
relevant_features  # উচ্চ কোরেলেশন ভেরিয়েবল দেখানো হচ্ছে

# ইনপুট (X) ও লক্ষ্য (Y) ভেরিয়েবল নির্ধারণ করা হচ্ছে
Y = df['Level'].values  # লক্ষ্য ভেরিয়েবল (Level) হিসেবে নেয়া হচ্ছে
X = df.drop('Level', axis=1).values  # ইনপুট ভেরিয়েবল হিসেবে 'Level' ছাড়া বাকি কলামগুলো নেয়া হচ্ছে

# ডেটাকে ট্রেনিং এবং টেস্টিং সেটে ভাগ করা হচ্ছে (80% ট্রেনিং, 20% টেস্টিং)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=21)  # ট্রেনিং এবং টেস্ট ডেটাতে ভাগ করা হচ্ছে

# মডেল প্রশিক্ষণের জন্য স্কোরিং পদ্ধতি নির্ধারণ করা হচ্ছে
scoring = 'accuracy'  # স্কোরিংয়ের জন্য 'accuracy' ব্যবহার করা হচ্ছে

# বিভিন্ন মডেল প্রস্তুত করা হচ্ছে
models = []  # মডেলগুলোর একটি তালিকা তৈরি হচ্ছে
models.append(('CART', DecisionTreeClassifier()))  # Decision Tree মডেল
models.append(('SVM', SVC()))  # SVM মডেল
models.append(('NB', GaussianNB()))  # Naive Bayes মডেল
models.append(('KNN', KNeighborsClassifier()))  # KNN মডেল

# মডেলগুলোর কার্যকারিতা (performance) যাচাই করা হচ্ছে
results = []  # ফলাফল সংরক্ষণের জন্য
names = []  # মডেলের নাম সংরক্ষণের জন্য

# কফোল্ড ক্রস ভ্যালিডেশন (10 ফোল্ড) এর মাধ্যমে প্রতিটি মডেল পরীক্ষা করা হচ্ছে
for name, model in models:
    kfold = KFold(n_splits=10)  # 10 ফোল্ড ক্রস ভ্যালিডেশন
    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)  # ক্রস ভ্যালিডেশন করা হচ্ছে
    results.append(cv_results)  # ফলাফল সংরক্ষণ করা হচ্ছে
    names.append(name)  # মডেলের নাম সংরক্ষণ করা হচ্ছে
    msg = "For %s Model: Mean accuracy is %f (Std accuracy is %f)" % (name, cv_results.mean(), cv_results.std())  # ফলাফল প্রদর্শন
    print(msg)  # মডেলের কার্যকারিতা মুদ্রণ

# মডেলগুলোর কার্যকারিতার তুলনা করার জন্য বক্সপ্লট তৈরি করা হচ্ছে
fig = plt.figure(figsize=(10,10))  # গ্রাফের আকার নির্ধারণ করা হচ্ছে
fig.suptitle('Performance Comparison')  # গ্রাফের শিরোনাম
ax = fig.add_subplot(111)  # সাবপ্লট তৈরি
plt.boxplot(results)  # বক্সপ্লট তৈরি
ax.set_xticklabels(names)  # মডেলের নাম হিসাবে লেবেল যোগ করা হচ্ছে
plt.show()  # বক্সপ্লট প্রদর্শন

# ডেটাকে স্কেল করার জন্য স্ট্যান্ডারাইজ করা হচ্ছে
import warnings
pipelines = []  # পাইপলাইনের তালিকা তৈরি

# প্রতিটি মডেলকে স্কেল এবং প্রশিক্ষণ করতে পাইপলাইন তৈরি করা হচ্ছে
pipelines.append(('Scaled CART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeClassifier())])))  # ডিসিশন ট্রি মডেলের পাইপলাইন
pipelines.append(('Scaled SVM', Pipeline([('Scaler', StandardScaler()),('SVM', SVC())])))  # SVM মডেলের পাইপলাইন
pipelines.append(('Scaled NB', Pipeline([('Scaler', StandardScaler()),('NB', GaussianNB())])))  # গাউসিয়ান বায়েস মডেলের পাইপলাইন
pipelines.append(('Scaled KNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsClassifier())])))  # KNN মডেলের পাইপলাইন

# স্কেল করা ডেটা নিয়ে মডেলগুলো পরীক্ষা করা হচ্ছে
results = []
names = []

kfold = KFold(n_splits=10)  # 10 ফোল্ড ক্রস ভ্যালিডেশন
for name, model in pipelines:
    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')  # ক্রস ভ্যালিডেশন করা হচ্ছে
    results.append(cv_results)
    names.append(name)
    print("For %s Model: Mean Accuracy is %f (Std Accuracy is %f)" % (name, cv_results.mean(), cv_results.std()))

# স্কেল করা ডেটার জন্য বক্সপ্লট তৈরি করা হচ্ছে
fig = plt.figure(figsize=(10,10))  # গ্রাফের আকার নির্ধারণ
fig.suptitle('Performance Comparison For Standarised Data')  # শিরোনাম
ax = fig.add_subplot(111)  # সাবপ্লট তৈরি
plt.boxplot(results)  # বক্সপ্লট তৈরি
ax.set_xticklabels(names)  # মডেলের নামের লেবেল যোগ করা হচ্ছে
plt.show()  # বক্সপ্লট প্রদর্শন

# মডেলটি প্রশিক্ষিত করার পরে টেস্ট ডেটার উপর প্রেডিকশন করা হচ্ছে
for name, model in models:
    model.fit(X_train, Y_train)  # মডেল প্রশিক্ষিত করা হচ্ছে
    predictions = model.predict(X_test)  # টেস্ট ডেটার উপর প্রেডিকশন করা হচ্ছে
    print("\nModel:", name)  # মডেলের নাম প্রদর্শন
    print("Accuracy score:", accuracy_score(Y_test, predictions))  # সঠিকতা স্কোর
    print("Classification report:\n", classification_report(Y_test, predictions))  # ক্লাসিফিকেশন রিপোর্ট

# মডেল নির্বাচন (SVC) করে প্রশিক্ষণ এবং টেস্ট করা হচ্ছে
clf = SVC()  # SVC মডেল

# মডেলটি প্রশিক্ষিত করা হচ্ছে
clf.fit(X_train, Y_train)  # প্রশিক্ষণ

# টেস্ট ডেটার উপর সঠিকতা যাচাই করা হচ্ছে
accuracy = clf.score(X_test, Y_test)  # সঠিকতা স্কোর
print("Test Accuracy:", accuracy)

# টেস্ট ডেটার উপর প্রেডিকশন করা হচ্ছে
predict = clf.predict(X_test)
predict  # প্রেডিকশন দেখানো হচ্ছে

# নতুন ডেটার উপর মডেল প্রেডিকশন
example_measures = [[35,1,4,5,6,5,5,4,6,7,2,3,4,8,8,7,9,2,1,4,6,7,2]]  # নতুন ডেটার উদাহরণ
prediction = clf.predict(example_measures)  # প্রেডিকশন করা হচ্ছে
print(prediction)  # প্রেডিকশন ফলাফল

# কনফিউশন ম্যাট্রিক্স তৈরির জন্য ফাংশন
import itertools
sns.set_theme(style="dark")  # সিবর্ন থিম সেট করা হচ্ছে
def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):
    """
    কনফিউশন ম্যাট্রিক্স প্রিন্ট ও প্লট করার জন্য এই ফাংশনটি ব্যবহার করা হচ্ছে।
    নরমালাইজেশন প্রয়োগ করতে normalize=True সেট করা যেতে পারে।
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # নরমালাইজেশন
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)  # কনফিউশন ম্যাট্রিক্স প্রদর্শন

    plt.imshow(cm, interpolation='nearest', cmap=cmap)  # কনফিউশন ম্যাট্রিক্সের হিটম্যাপ
    plt.title(title)  # শিরোনাম
    plt.colorbar()  # কালার বার
    tick_marks = np.arange(len(classes))  # ক্লাস লেবেল তৈরি
    plt.xticks(tick_marks, classes, rotation=45)  # এক্স টিক চিহ্ন
    plt.yticks(tick_marks, classes)  # ওয়াই টিক চিহ্ন

    fmt = '.2f' if normalize else 'd'  # ফরম্যাট সেট করা হচ্ছে
    thresh = cm.max() / 2.  # থ্রেশোল্ড নির্ধারণ
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment="center", verticalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")  # সেলটিতে মান যোগ করা হচ্ছে

    plt.ylabel('True label')  # ইয় আউটপুট লেবেল
    plt.xlabel('Predicted label')  # এক্স আউটপুট লেবেল
    plt.tight_layout()  # লেআউট ঠিক করা হচ্ছে

# কনফিউশন ম্যাট্রিক্স প্লট করা হচ্ছে
cm = confusion_matrix(Y_test, predictions)  # কনফিউশন ম্যাট্রিক্স তৈরি
plot_confusion_matrix(cm, classes=[1, 2, 3], normalize=True, title='Normalized confusion matrix')  # কনফিউশন ম্যাট্রিক্স প্লট




